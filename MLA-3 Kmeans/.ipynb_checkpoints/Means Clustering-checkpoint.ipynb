{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YqaE2-eQ5iz"
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Let’s suppose we give a child different objects to group. How does a child make a group? The child may group over the colour, over the shape, over the hardness or softness of the objects etc. The basic idea here is that the child tries to find out similarities and dissimilarities between different objects and then tries to make a group of similar objects. This is called **clustering**, the method of identifying similar instances and keeping them together.\n",
    "In Other words, clustering identifies homogeneous subgroups among the observations.\n",
    "\n",
    "\n",
    "Clustering is an unsupervised approach which finds a structure/pattern in a collection of unlabeled data.\n",
    "A cluster is a collection of objects which are “similar” amongst themselves and are “dissimilar” to the objects belonging to a different cluster.\n",
    "For example:\n",
    "\n",
    "\n",
    "<img src=\"https://summerofhpc.prace-ri.eu/wp-content/uploads/2013/07/k-means_ilustration.png\">\n",
    "\n",
    "\n",
    "In the figure above, we can easily identify 4 different clusters. The clustering criteria here is distance. Whichever points are near to each other are kept in the same cluster and the faraway points belong to a different cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGTXuo4dQ5i7"
   },
   "source": [
    "# KMeans Clustering\n",
    "\n",
    "It is an unsupervised technique used to identify clusters of data objects in the dataset. K specifies the number of clusters.\n",
    "\n",
    "Steps involved in K-Means Algorithm:\n",
    "\n",
    "1. Initially choose the number of K clusters.\n",
    "\n",
    "2. Start with K centroids by putting them at random place (not necessarily from your dataset). **A centroid is a data point (imaginary or real) at the center of a cluster.**\n",
    "\n",
    "3. Assign each point to the closest centroid. That forms k cluster. Compute distance of every point from centroid.\n",
    "\n",
    "4. Compute and place the new centroid for each cluster ie., calculate the mean value of the objects for each cluster and update the cluster mean.\n",
    "\n",
    "5. Repeat the process again ie., reassign each data point to the new closest centroid. If any reassignment took place, go to step 4. But if there is no change ie., when clusters form a clear boundary, then stop.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00ussm7BQ5i9"
   },
   "source": [
    "## Problem statement: With the iris dataset we need to cluster iris flowers to which species it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxWSvQ6sQ5i-"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAb9q1nYQ5jC",
    "outputId": "3c655765-5876-43f7-ac33-e73135ef13a0"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "# checking the first 5 rows of our dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtpT_Qa5Q5jE",
    "outputId": "a46380c3-c8f6-49f2-9d40-315e40ca11b7"
   },
   "outputs": [],
   "source": [
    "# Quick summary of the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=data[['sepal_length', 'sepal_width', 'petal_length','petal_width']]\n",
    "# Creating dataframe only for numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bJ3rPd7Q5jF",
    "outputId": "838b82f5-661e-445e-f605-db762999fc2a"
   },
   "outputs": [],
   "source": [
    "# Statistical information of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAhF5e0sQ5jG",
    "outputId": "5bee0bc9-6ad6-444f-bce8-a00edc634e4a"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bm1R6F4Q5jH"
   },
   "outputs": [],
   "source": [
    "# there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL69UzVUQ5jI"
   },
   "outputs": [],
   "source": [
    "# Drop the unnamed column in the dataset\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeP23FLlQ5jL",
    "outputId": "854e0455-a1e5-44a4-a8a0-a19308ef5aa2"
   },
   "outputs": [],
   "source": [
    "# Since it is an unsupervised learning, we need only the input variables\n",
    "X= data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT5ZRoL0Q5jN",
    "outputId": "2409a0e4-2c2b-4408-9529-35fc46de1643"
   },
   "outputs": [],
   "source": [
    "# Import KMeans Algorithm from Scikit-learn library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Initialise the model with K as 3\n",
    "model = KMeans(n_clusters=3, random_state=10)\n",
    "\n",
    "#Training the model\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPPdgDPNQ5jO",
    "outputId": "25dc9ceb-258c-4eef-f450-6b0b8023a41e"
   },
   "outputs": [],
   "source": [
    "# labels of the clusters\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3tCdJScQ5jR",
    "outputId": "23aa425a-b1d3-4959-a945-3dcae5671fcb"
   },
   "outputs": [],
   "source": [
    "# centroids of the clusters\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn4dBo8zQ5jS"
   },
   "outputs": [],
   "source": [
    "# Set colours to the clusters to differentiate\n",
    "color_scheme = np.array(['red','blue','green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11jBX-UuQ5jU",
    "outputId": "0d22d998-52db-4d9c-fb7e-b2701499d6cd"
   },
   "outputs": [],
   "source": [
    "# Visualize the clusters in the original data\n",
    "plt.scatter(data.petal_length,data.petal_width,color=color_scheme[data.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M85b6j-HQ5jV",
    "outputId": "716552e5-dbb7-4251-8031-c4973d307518"
   },
   "outputs": [],
   "source": [
    "# visualize the clusters formed by the model\n",
    "plt.scatter(X.petal_length,X.petal_width,color=color_scheme[model.labels_]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ImVevdiQ5jW",
    "outputId": "a82951ff-9af4-4b9d-a3c7-6a4fddc56f81"
   },
   "outputs": [],
   "source": [
    "# print dataset with the label\n",
    "X['Group']= pd.DataFrame(model.labels_)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_point=[[3.4,5.6,2.4,0.8]] # A new observation where it will go \n",
    "#means which cluster\n",
    "model.predict(new_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCrCrAsuQ5jW"
   },
   "source": [
    "# Elbow Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1cObXlDQ5jY"
   },
   "source": [
    "Elbow method is the most popular method that is used to determine the optimal value of k.\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "- Start with some K.\n",
    "- Calculate WCSS (Within Cluster Sum of squared errors) ie., for each of the cluster, it calculate the distance of individual data points from the centroid, then square it and sum it up.\n",
    "             WCSS = WCSS1 + WCSS2 + ..... WCSSk\n",
    "             \n",
    "- Take new value for K, repeat step2. \n",
    "- For each number of K, WCSS is calculated. \n",
    "- Find the elbow point. That is the optimal value of K.\n",
    "\n",
    "![WCSS.png](attachment:WCSS.png)\n",
    " As the number of K increases, the error reduces. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv9bg5o5Q5jZ"
   },
   "source": [
    "### Determine K using elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJDtYe53Q5ja",
    "outputId": "cb938e79-285e-4e53-f8f3-5bed4e52ed26"
   },
   "outputs": [],
   "source": [
    "#elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss=[]\n",
    "for i in range (3,11):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(3,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKGZZIHCQ5jc"
   },
   "source": [
    "To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie., the point after which the distortion/inertia start decreasing in a linear fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZirHFGVQ5jd"
   },
   "source": [
    "### Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCHL2rfJQ5jd",
    "outputId": "92cf6288-366f-4829-bf72-f993908df989"
   },
   "outputs": [],
   "source": [
    "label=model.labels_\n",
    "label\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score=silhouette_score(X,label)\n",
    "score #K=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlZrcV5tQ5je"
   },
   "source": [
    "The range of Silhouette score is [-1, 1]. Its analysis is as follows −\n",
    "\n",
    "+1 Score − Near +1 Silhouette score indicates that the sample is far away from its neighboring cluster.\n",
    "\n",
    "0 Score − 0 Silhouette score indicates that the sample is on or very close to the decision boundary separating two neighboring clusters.\n",
    "\n",
    "-1 Score − 1 Silhouette score indicates that the samples have been assigned to the wrong clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjADNjkTQ5jg"
   },
   "source": [
    "The calculation of Silhouette score can be done by using the following formula\n",
    "\n",
    "$$silhouette score\\:=\\:(p-q)/max(p,q)$$\n",
    "\n",
    "Here, p = mean distance to the points in the nearest cluster\n",
    "\n",
    "And, q = mean intra-cluster distance to all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMtuF828Q5jh"
   },
   "source": [
    "### Applications\n",
    "The scikit-learn book describes the various applications of clustering as follows:\n",
    "\n",
    "* __For customer segmentation:__ You can cluster your customers based on their purchases,their activity on your website, and so on. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, this can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.Subdivision of customers into groups/segments such that each customer segment consists of customers with similar market characteristics — pricing , loyalty, spending behaviors etc. Some of the segmentation variables could be e.g., number of items bought on sale, avg transaction value, total number of transactions. Customer segmentation allows businesses to customize market programs that will be suitable for each of its customer segments\n",
    "\n",
    "* __For data analysis:__ When analyzing a new dataset, it is often useful to first discover clusters of similar instances, as it is often easier to analyze clusters separately.\n",
    "\n",
    "* __Inventory Categorization based on sales or other manufacturing metrics.\n",
    "\n",
    "* __For anomaly detection (also called outlier detection):__ Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second, and so on. Anomaly detection is particularly useful in detecting defects in manufacturing, or for fraud detection.\n",
    "\n",
    "\n",
    "* __For search engines:__ For example, some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database: similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is to find this image’s cluster using the trained clustering model, and you can then simply return all the images from this cluster.\n",
    "\n",
    "* __To segment an image:__ By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to reduce the number of different colors in the image considerably. This technique is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdV8nN6xQ5ji"
   },
   "source": [
    "##### Approaches for Clustering:\n",
    "The clustering approaches can be broadly divided into two categories: _Agglomerative_ and _Divisive_.\n",
    "\n",
    "**Agglomerative:** This approach first considers all the points as individual clusters and then finds out the similarity between two points, puts them into a cluster. Then it goes on finding similar points and clusters until there is only one cluster left i.e., all points belong to a big cluster. This is also called the bottom-up approach.\n",
    "\n",
    "**Divisive:** It is opposite of the agglomerative approach. It first considers all the points to be part of one big cluster and in the subsequent steps tries to find out the points/ clusters which are least similar to each other and then breaks the bigger cluster into smaller ones. This continues until there are as many clusters as there are datapoints. This is also called the top-down approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn6yrn6nQ5jj"
   },
   "source": [
    "### Assumption of K Means:\n",
    "- Clusters are spatially grouped or spherical\n",
    "- All features are scaled\n",
    "- Clusters are of similar size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNOZ37rQQ5jl"
   },
   "source": [
    "### Pros:\n",
    "- Works well even when some assumptions are broken\n",
    "- Simple and easy to implement\n",
    "- Easy to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILCeElwQ5jo"
   },
   "source": [
    "### Cons:\n",
    "\n",
    "- Sensitive to outliers\n",
    "- k value has to be known before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4xbjZeWQ5jr"
   },
   "source": [
    "### Goal of Clustering\n",
    "\n",
    "- Distance between centroid and its point should be minimum\n",
    "- Intra-cluster distance should be less\n",
    "- Distance between intercluster should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGapdEWIQ5js"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oT_j-9EWQ5jt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Means Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
